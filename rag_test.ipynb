{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RagDataLoader import RAG_DataLoader, TextSplitterSelector, TokenizerSelector\n",
    "from LLMLoader import  TestLLM\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda':\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "print('Device: ',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beansamuel/anaconda3/envs/test/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataloader = RAG_DataLoader(\n",
    "        tokenize_model = TokenizerSelector.all_MiniLM_L6_v2,\n",
    "        chunk_size = 500,\n",
    "        chunk_overlap = 25,\n",
    "        textsplitter = TextSplitterSelector.Chroma_RecursiveCharacterTextSplitter,\n",
    "        db_path = 'arvix_finance_db_all_MiniLM_L6_v2',\n",
    "        device = device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 369/1000 [00:24<00:40, 15.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file data/arxiv_finance/2305.08235.pdf: Failed to open file 'data/arxiv_finance/2305.08235.pdf'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 561/1000 [00:37<00:23, 18.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file data/arxiv_finance/2004.13261.pdf: Failed to open file 'data/arxiv_finance/2004.13261.pdf'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 632/1000 [00:40<00:15, 23.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: syntax error: could not parse color space (324 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (613 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (792 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (829 0 R)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 672/1000 [00:44<00:22, 14.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: syntax error: unknown keyword: 'pagesize'\n",
      "\n",
      "MuPDF error: syntax error: unknown keyword: 'width'\n",
      "\n",
      "MuPDF error: syntax error: unknown keyword: '597.50787pt'\n",
      "\n",
      "MuPDF error: syntax error: unknown keyword: 'height'\n",
      "\n",
      "MuPDF error: syntax error: unknown keyword: '845.04684pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 675/1000 [00:44<00:19, 16.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: syntax error: unknown keyword: 'pagesize'\n",
      "\n",
      "MuPDF error: syntax error: unknown keyword: 'width'\n",
      "\n",
      "MuPDF error: syntax error: unknown keyword: '614.295pt'\n",
      "\n",
      "MuPDF error: syntax error: unknown keyword: 'height'\n",
      "\n",
      "MuPDF error: syntax error: unknown keyword: '794.96999pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|██████▊   | 685/1000 [00:45<00:12, 24.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file data/arxiv_finance/1703.02113.pdf: Failed to open file 'data/arxiv_finance/1703.02113.pdf'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 781/1000 [00:51<00:09, 24.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file data/arxiv_finance/1002.1994.pdf: Failed to open file 'data/arxiv_finance/1002.1994.pdf'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [01:08<00:00, 14.69it/s]\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'data/arxiv_finance'\n",
    "\n",
    "for item in tqdm(os.listdir(folder_path)[:1000]):\n",
    "    dataloader.add(os.path.join(folder_path,item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beansamuel/anaconda3/envs/test/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "dataloader.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.add('data/rag_text_mental.pdf')\n",
    "dataloader.add('data/rag_text.pdf')\n",
    "dataloader.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from model/TAIDE-LX-7B.f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 56064\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,56064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,56064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,56064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 304/56064 vs 303/56064 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 56064\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.94 B\n",
      "llm_load_print_meta: model size       = 12.92 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size = 13229.02 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.21 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.model': 'llama', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.vocab_size': '56064', 'general.file_type': '1', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32'}\n",
      "Using fallback chat format: llama-2\n",
      "/home/beansamuel/anaconda3/envs/test/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = TestLLM( model_path = 'model/TAIDE-LX-7B.f16.gguf' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beansamuel/anaconda3/envs/test/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "在古希臘哲學中, 柏拉圖提出三種人格, 並認為人是由理性和欲望構成的, 其中最接近理性的部分被稱為靈魂; 而靈魂又可分為三部分: 本我(Id), 自我(Ego)和超我(Superego)。\n",
      "這三者是構成人的基礎, 也是道德意識的基礎。在現代心理學中, 這個概念也被廣泛應用於解釋人格的形成與發展。然而, 有關「本我、自我、超我」的概念究竟是誰提出的?答案可能出乎你意料之外!\n",
      "「本我」和「自我」是佛洛伊德最早提出的概念,而「超我」則是後來才由榮格(Carl Gustav Jung)引入的。\n",
      "自古以來, 心理學就一直存在著「人就是動物」的假設,認為人的行為可以歸因於生物因素和環境因素等外部力量的影響。然而, 到了20世紀初, 隨著精神分析學派的發展, 人們開始認識到, 人類行為並非僅僅由外在的刺激或環境所決定; 人類的心理活動也同樣具有自主性, 這種自主性的"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   21822.28 ms\n",
      "llama_print_timings:      sample time =      90.41 ms /   256 runs   (    0.35 ms per token,  2831.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21822.19 ms /    25 tokens (  872.89 ms per token,     1.15 tokens per second)\n",
      "llama_print_timings:        eval time =   39554.30 ms /   255 runs   (  155.11 ms per token,     6.45 tokens per second)\n",
      "llama_print_timings:       total time =   62373.24 ms /   280 tokens\n"
     ]
    }
   ],
   "source": [
    "llm.chat('「本我(Id)、自我(Ego)、超我(Superego)」是誰提出的?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "我是個對哲學很陌生的人，所以對於這個東西其實不太懂。不過，我想起在國中時曾接觸過一些哲學的東西。當時我們班上某位同學因為不喜歡老師，便自己上網找了一些資料來教大家。她曾提到：「本我、自我和超我」。我那時對這個名詞還不是很清楚，只記得就是人的三種人格特質。\n",
      "後來在高中時又再次聽到「本我、自我、超我」，不過，因為老師當時沒講得很詳細，所以對於這三個名詞還是有點模糊。因此我便上網找了一些資料來了解它們的差別。後來發現其實這三個名詞有很大的不同，而其概念也影響了哲學家的思想。\n",
      "以下就來說說三者之間的關係：\n",
      "本我(Id)：是人出生時最原始的部分，也是人的慾望、衝動和本能。它只對立即滿足的需求有所反應，且不受控制，所以常會違背道德或社會規範而做出傷害的行為。\n",
      "自我(Ego):「自我」是一個有感情的人格結構。它是透過學習和觀察而形成的。在成長的過程中，我們透過與父母和其他成人接觸來獲得對他人的"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   29002.93 ms\n",
      "llama_print_timings:      sample time =      89.03 ms /   256 runs   (    0.35 ms per token,  2875.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   28180.56 ms /    32 tokens (  880.64 ms per token,     1.14 tokens per second)\n",
      "llama_print_timings:        eval time =   39915.25 ms /   255 runs   (  156.53 ms per token,     6.39 tokens per second)\n",
      "llama_print_timings:       total time =   69034.77 ms /   287 tokens\n"
     ]
    }
   ],
   "source": [
    "llm.chat_prompt_qa('「本我(Id)、自我(Ego)、超我(Superego)」是誰提出的?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      " 亞里斯多德。\n",
      "\n",
      "Yi’s, i = 1, ..., m. This model can also be used to develop Bayes predictors of θi’s. For small\n",
      "\n",
      "個可安穩的刺激致中，使我們能集中注意力在有限的範圍環境中。這也能從側面上\n",
      "驗證為何青少年傾向於滑手機到睡著前的最後一刻，因為這樣能獲的安全感和從\n",
      "中獲得少許得快樂。\n",
      "和動作，使「自我」超越了「本我」和「超我」，而消弭了疼痛以及減少生理上的慾望，\n",
      "因此在網路上時常可以看到練瑜珈的人能將身體超出常理的翻摺或是禁食很長一\n",
      "段時間。\n",
      "本書希望「自我」佔領身體的主導權，主要由於進入此狀態通常是需要有意識地將\n",
      "人類的肉體和心智發揮到極限，去完成某件也難度或有價值的事情，進而締造出\n",
      "「最優體驗」並從中獲得純粹的快樂，而其中有意識地將肉體和心智發揮到極致的\n",
      "狀態就是心流狀態。因此以心流狀態創造最優體驗或許"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   31460.71 ms\n",
      "llama_print_timings:      sample time =      88.03 ms /   256 runs   (    0.34 ms per token,  2908.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =   61288.86 ms /   839 tokens (   73.05 ms per token,    13.69 tokens per second)\n",
      "llama_print_timings:        eval time =   34103.40 ms /   255 runs   (  133.74 ms per token,     7.48 tokens per second)\n",
      "llama_print_timings:       total time =   96354.32 ms /  1094 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "llm.chat_rag('「本我(Id)、自我(Ego)、超我(Superego)」是誰提出的?',dataloader.get_chromadb())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for StuffDocumentsChain\n__root__\n  document_variable_name context was not found in llm_chain input_variables: ['question'] (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_promptrag\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m「本我(Id)、自我(Ego)、超我(Superego)」是誰提出的?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chromadb\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rag_llama2-7b-chat/LLMLoader.py:101\u001b[0m, in \u001b[0;36mTestLLM.chat_promptrag\u001b[0;34m(self, query, vectordb)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat_promptrag\u001b[39m(\u001b[38;5;28mself\u001b[39m,query,vectordb):\n\u001b[1;32m    100\u001b[0m     retriever \u001b[38;5;241m=\u001b[39m vectordb\u001b[38;5;241m.\u001b[39mas_retriever()\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_retriever \u001b[38;5;241m=\u001b[39m \u001b[43mRetrievalQA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_chain_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstuff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchain_type_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mPromptSelector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefualt_rag_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_retriever\u001b[38;5;241m.\u001b[39minvoke(query)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/langchain/chains/retrieval_qa/base.py:106\u001b[0m, in \u001b[0;36mBaseRetrievalQA.from_chain_type\u001b[0;34m(cls, llm, chain_type, chain_type_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load chain from chain type.\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m _chain_type_kwargs \u001b[38;5;241m=\u001b[39m chain_type_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 106\u001b[0m combine_documents_chain \u001b[38;5;241m=\u001b[39m \u001b[43mload_qa_chain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_chain_type_kwargs\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(combine_documents_chain\u001b[38;5;241m=\u001b[39mcombine_documents_chain, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/langchain/chains/question_answering/chain.py:249\u001b[0m, in \u001b[0;36mload_qa_chain\u001b[0;34m(llm, chain_type, verbose, callback_manager, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chain_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_mapping:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported chain type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_mapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m     )\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchain_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/langchain/chains/question_answering/chain.py:81\u001b[0m, in \u001b[0;36m_load_stuff_chain\u001b[0;34m(llm, prompt, document_variable_name, verbose, callback_manager, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[1;32m     74\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     75\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m_prompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m     79\u001b[0m )\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# TODO: document prompt\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStuffDocumentsChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocument_variable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for StuffDocumentsChain\n__root__\n  document_variable_name context was not found in llm_chain input_variables: ['question'] (type=value_error)"
     ]
    }
   ],
   "source": [
    "llm.chat_promptrag('「本我(Id)、自我(Ego)、超我(Superego)」是誰提出的?',dataloader.get_chromadb())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='□\\n29', metadata={'author': '', 'creationDate': \"D:20220304071224-05'00'\", 'creator': 'LaTeX with hyperref package', 'file_path': 'data/arxiv_finance/1610.08909.pdf', 'format': 'PDF 1.4', 'keywords': '', 'modDate': \"D:20220304071224-05'00'\", 'page': 28, 'producer': 'dvips + GPL Ghostscript GIT PRERELEASE 9.22', 'source': 'data/arxiv_finance/1610.08909.pdf', 'subject': '', 'title': '', 'total_pages': 50, 'trapped': ''}),\n",
       " Document(page_content='(2,𝑛) + 1| −|𝜁𝑘\\n(2,𝑛)|)} √∆𝑡] ,', metadata={'author': 'svetlozar rachev', 'creationDate': \"D:20230329152933-05'00'\", 'creator': 'Microsoft® Word 2019', 'file_path': 'data/arxiv_finance/2303.17014.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': \"D:20230329152933-05'00'\", 'page': 20, 'producer': 'Microsoft® Word 2019', 'source': 'data/arxiv_finance/2303.17014.pdf', 'subject': '', 'title': '', 'total_pages': 49, 'trapped': ''}),\n",
       " Document(page_content='(15)\\nzr\\nx(a) := (1−r)ux(a)\\nux\\n+rmx(a) ∏\\nad∈a\\nux(ad)\\nux\\n.\\n19', metadata={'author': 'Srinivas Arigapudi ; Omer Edhan ; Yuval Heller ; Ziv Hellman', 'creationDate': 'D:20231114033845Z', 'creator': 'LaTeX with hyperref', 'file_path': 'data/arxiv_finance/2205.00278.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20231114033845Z', 'page': 18, 'producer': 'pdfTeX-1.40.25', 'source': 'data/arxiv_finance/2205.00278.pdf', 'subject': '', 'title': 'Mentors and Recombinators: Multi-Dimensional Social Learning', 'total_pages': 58, 'trapped': ''}),\n",
       " Document(page_content='ϵ0, ˜\\nϵ−1, . . . do we prefer to use a different letter for them. Note\\n15', metadata={'author': 'Beutner Heinemann Smeekes', 'creationDate': 'D:20230816005521Z', 'creator': 'LaTeX with hyperref', 'file_path': 'data/arxiv_finance/1808.09125.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20230816005521Z', 'page': 14, 'producer': 'pdfTeX-1.40.25', 'source': 'data/arxiv_finance/1808.09125.pdf', 'subject': '', 'title': 'Bootstrapping Risk Measures', 'total_pages': 117, 'trapped': ''})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.chroma_db.similarity_search('跟我推薦財經相關論文')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TokenizerSelector Comparison\n",
    "Language Comparison\n",
    "Model Comparison\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
