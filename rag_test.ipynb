{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RagDataLoader import RAG_DataLoader, TextSplitterSelector, TokenizerSelector\n",
    "from LLMLoader import  TestLLM\n",
    "import os\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:  cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "if device == 'cuda':\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "print('Device: ',device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beansamuel/anaconda3/envs/test/lib/python3.9/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "dataloader = RAG_DataLoader(\n",
    "        tokenize_model = TokenizerSelector.all_MiniLM_L6_v2,\n",
    "        chunk_size = 500,\n",
    "        chunk_overlap = 25,\n",
    "        textsplitter = TextSplitterSelector.Chroma_RecursiveCharacterTextSplitter,\n",
    "        db_path = 'arvix_finance_db_all_MiniLM_L6_v2',\n",
    "        device = device,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|‚ñà‚ñà‚ñà‚ñã      | 369/1000 [00:24<00:40, 15.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file data/arxiv_finance/2305.08235.pdf: Failed to open file 'data/arxiv_finance/2305.08235.pdf'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 561/1000 [00:37<00:23, 18.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file data/arxiv_finance/2004.13261.pdf: Failed to open file 'data/arxiv_finance/2004.13261.pdf'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 632/1000 [00:40<00:15, 23.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: syntax error: could not parse color space (324 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (613 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (792 0 R)\n",
      "\n",
      "MuPDF error: syntax error: could not parse color space (829 0 R)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 672/1000 [00:44<00:22, 14.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: syntax error: unknown keyword: 'pagesize'\n",
      "\n",
      "MuPDF error: syntax error: unknown keyword: 'width'\n",
      "\n",
      "MuPDF error: syntax error: unknown keyword: '597.50787pt'\n",
      "\n",
      "MuPDF error: syntax error: unknown keyword: 'height'\n",
      "\n",
      "MuPDF error: syntax error: unknown keyword: '845.04684pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 675/1000 [00:44<00:19, 16.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MuPDF error: syntax error: unknown keyword: 'pagesize'\n",
      "\n",
      "MuPDF error: syntax error: unknown keyword: 'width'\n",
      "\n",
      "MuPDF error: syntax error: unknown keyword: '614.295pt'\n",
      "\n",
      "MuPDF error: syntax error: unknown keyword: 'height'\n",
      "\n",
      "MuPDF error: syntax error: unknown keyword: '794.96999pt'\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 685/1000 [00:45<00:12, 24.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file data/arxiv_finance/1703.02113.pdf: Failed to open file 'data/arxiv_finance/1703.02113.pdf'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 781/1000 [00:51<00:09, 24.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing file data/arxiv_finance/1002.1994.pdf: Failed to open file 'data/arxiv_finance/1002.1994.pdf'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [01:08<00:00, 14.69it/s]\n"
     ]
    }
   ],
   "source": [
    "folder_path = 'data/arxiv_finance'\n",
    "\n",
    "for item in tqdm(os.listdir(folder_path)[:1000]):\n",
    "    dataloader.add(os.path.join(folder_path,item))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beansamuel/anaconda3/envs/test/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: Since Chroma 0.4.x the manual persistence method is no longer supported as docs are automatically persisted.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "dataloader.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader.add('data/rag_text_mental.pdf')\n",
    "dataloader.add('data/rag_text.pdf')\n",
    "dataloader.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 21 key-value pairs and 291 tensors from model/TAIDE-LX-7B.f16.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
      "llama_model_loader: - kv   2:                           llama.vocab_size u32              = 56064\n",
      "llama_model_loader: - kv   3:                       llama.context_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   5:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   6:                  llama.feed_forward_length u32              = 11008\n",
      "llama_model_loader: - kv   7:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   8:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   9:              llama.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  10:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  11:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  12:                          general.file_type u32              = 1\n",
      "llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = llama\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: - kv  14:                      tokenizer.ggml.tokens arr[str,56064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  15:                      tokenizer.ggml.scores arr[f32,56064]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,56064]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  20:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type  f16:  226 tensors\n",
      "llm_load_vocab: mismatch in special tokens definition ( 304/56064 vs 303/56064 ).\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = llama\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 56064\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: n_ctx_train      = 4096\n",
      "llm_load_print_meta: n_embd           = 4096\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_rot            = 128\n",
      "llm_load_print_meta: n_embd_head_k    = 128\n",
      "llm_load_print_meta: n_embd_head_v    = 128\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
      "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 11008\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 0\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: model type       = 8B\n",
      "llm_load_print_meta: model ftype      = F16\n",
      "llm_load_print_meta: model params     = 6.94 B\n",
      "llm_load_print_meta: model size       = 12.92 GiB (16.00 BPW) \n",
      "llm_load_print_meta: general.name     = LLaMA v2\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 2 '</s>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_tensors: ggml ctx size =    0.15 MiB\n",
      "llm_load_tensors:        CPU buffer size = 13229.02 MiB\n",
      "................................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 2048\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1024.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1024.00 MiB, K (f16):  512.00 MiB, V (f16):  512.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.21 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   164.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1030\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 1 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.ggml.eos_token_id': '2', 'tokenizer.ggml.model': 'llama', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.vocab_size': '56064', 'general.file_type': '1', 'tokenizer.ggml.add_bos_token': 'true', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32'}\n",
      "Using fallback chat format: llama-2\n",
      "/home/beansamuel/anaconda3/envs/test/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 0.3.0. Use RunnableSequence, e.g., `prompt | llm` instead.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "source": [
    "llm = TestLLM( model_path = 'model/TAIDE-LX-7B.f16.gguf' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/beansamuel/anaconda3/envs/test/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:119: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Âú®Âè§Â∏åËáòÂì≤Â≠∏‰∏≠, ÊüèÊãâÂúñÊèêÂá∫‰∏âÁ®Æ‰∫∫Ê†º, ‰∏¶Ë™çÁÇ∫‰∫∫ÊòØÁî±ÁêÜÊÄßÂíåÊ¨≤ÊúõÊßãÊàêÁöÑ, ÂÖ∂‰∏≠ÊúÄÊé•ËøëÁêÜÊÄßÁöÑÈÉ®ÂàÜË¢´Á®±ÁÇ∫ÈùàÈ≠Ç; ËÄåÈùàÈ≠ÇÂèàÂèØÂàÜÁÇ∫‰∏âÈÉ®ÂàÜ: Êú¨Êàë(Id), Ëá™Êàë(Ego)ÂíåË∂ÖÊàë(Superego)„ÄÇ\n",
      "ÈÄô‰∏âËÄÖÊòØÊßãÊàê‰∫∫ÁöÑÂü∫Á§é, ‰πüÊòØÈÅìÂæ∑ÊÑèË≠òÁöÑÂü∫Á§é„ÄÇÂú®Áèæ‰ª£ÂøÉÁêÜÂ≠∏‰∏≠, ÈÄôÂÄãÊ¶ÇÂøµ‰πüË¢´Âª£Ê≥õÊáâÁî®ÊñºËß£Èáã‰∫∫Ê†ºÁöÑÂΩ¢ÊàêËàáÁôºÂ±ï„ÄÇÁÑ∂ËÄå, ÊúâÈóú„ÄåÊú¨Êàë„ÄÅËá™Êàë„ÄÅË∂ÖÊàë„ÄçÁöÑÊ¶ÇÂøµÁ©∂Á´üÊòØË™∞ÊèêÂá∫ÁöÑ?Á≠îÊ°àÂèØËÉΩÂá∫‰πé‰Ω†ÊÑèÊñô‰πãÂ§ñ!\n",
      "„ÄåÊú¨Êàë„ÄçÂíå„ÄåËá™Êàë„ÄçÊòØ‰ΩõÊ¥õ‰ºäÂæ∑ÊúÄÊó©ÊèêÂá∫ÁöÑÊ¶ÇÂøµ,ËÄå„ÄåË∂ÖÊàë„ÄçÂâáÊòØÂæå‰æÜÊâçÁî±Ê¶ÆÊ†º(Carl Gustav Jung)ÂºïÂÖ•ÁöÑ„ÄÇ\n",
      "Ëá™Âè§‰ª•‰æÜ, ÂøÉÁêÜÂ≠∏Â∞±‰∏ÄÁõ¥Â≠òÂú®Ëëó„Äå‰∫∫Â∞±ÊòØÂãïÁâ©„ÄçÁöÑÂÅáË®≠,Ë™çÁÇ∫‰∫∫ÁöÑË°åÁÇ∫ÂèØ‰ª•Ê≠∏Âõ†ÊñºÁîüÁâ©Âõ†Á¥†ÂíåÁí∞Â¢ÉÂõ†Á¥†Á≠âÂ§ñÈÉ®ÂäõÈáèÁöÑÂΩ±Èüø„ÄÇÁÑ∂ËÄå, Âà∞‰∫Ü20‰∏ñÁ¥ÄÂàù, Èö®ËëóÁ≤æÁ•ûÂàÜÊûêÂ≠∏Ê¥æÁöÑÁôºÂ±ï, ‰∫∫ÂÄëÈñãÂßãË™çË≠òÂà∞, ‰∫∫È°ûË°åÁÇ∫‰∏¶ÈùûÂÉÖÂÉÖÁî±Â§ñÂú®ÁöÑÂà∫ÊøÄÊàñÁí∞Â¢ÉÊâÄÊ±∫ÂÆö; ‰∫∫È°ûÁöÑÂøÉÁêÜÊ¥ªÂãï‰πüÂêåÊ®£ÂÖ∑ÊúâËá™‰∏ªÊÄß, ÈÄôÁ®ÆËá™‰∏ªÊÄßÁöÑ"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   21822.28 ms\n",
      "llama_print_timings:      sample time =      90.41 ms /   256 runs   (    0.35 ms per token,  2831.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =   21822.19 ms /    25 tokens (  872.89 ms per token,     1.15 tokens per second)\n",
      "llama_print_timings:        eval time =   39554.30 ms /   255 runs   (  155.11 ms per token,     6.45 tokens per second)\n",
      "llama_print_timings:       total time =   62373.24 ms /   280 tokens\n"
     ]
    }
   ],
   "source": [
    "llm.chat('„ÄåÊú¨Êàë(Id)„ÄÅËá™Êàë(Ego)„ÄÅË∂ÖÊàë(Superego)„ÄçÊòØË™∞ÊèêÂá∫ÁöÑ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ÊàëÊòØÂÄãÂ∞çÂì≤Â≠∏ÂæàÈôåÁîüÁöÑ‰∫∫ÔºåÊâÄ‰ª•Â∞çÊñºÈÄôÂÄãÊù±Ë•øÂÖ∂ÂØ¶‰∏çÂ§™ÊáÇ„ÄÇ‰∏çÈÅéÔºåÊàëÊÉ≥Ëµ∑Âú®Âúã‰∏≠ÊôÇÊõæÊé•Ëß∏ÈÅé‰∏Ä‰∫õÂì≤Â≠∏ÁöÑÊù±Ë•ø„ÄÇÁï∂ÊôÇÊàëÂÄëÁè≠‰∏äÊüê‰ΩçÂêåÂ≠∏Âõ†ÁÇ∫‰∏çÂñúÊ≠°ËÄÅÂ∏´Ôºå‰æøËá™Â∑±‰∏äÁ∂≤Êâæ‰∫Ü‰∏Ä‰∫õË≥áÊñô‰æÜÊïôÂ§ßÂÆ∂„ÄÇÂ•πÊõæÊèêÂà∞Ôºö„ÄåÊú¨Êàë„ÄÅËá™ÊàëÂíåË∂ÖÊàë„Äç„ÄÇÊàëÈÇ£ÊôÇÂ∞çÈÄôÂÄãÂêçË©ûÈÇÑ‰∏çÊòØÂæàÊ∏ÖÊ•öÔºåÂè™Ë®òÂæóÂ∞±ÊòØ‰∫∫ÁöÑ‰∏âÁ®Æ‰∫∫Ê†ºÁâπË≥™„ÄÇ\n",
      "Âæå‰æÜÂú®È´ò‰∏≠ÊôÇÂèàÂÜçÊ¨°ËÅΩÂà∞„ÄåÊú¨Êàë„ÄÅËá™Êàë„ÄÅË∂ÖÊàë„ÄçÔºå‰∏çÈÅéÔºåÂõ†ÁÇ∫ËÄÅÂ∏´Áï∂ÊôÇÊ≤íË¨õÂæóÂæàË©≥Á¥∞ÔºåÊâÄ‰ª•Â∞çÊñºÈÄô‰∏âÂÄãÂêçË©ûÈÇÑÊòØÊúâÈªûÊ®°Á≥ä„ÄÇÂõ†Ê≠§Êàë‰æø‰∏äÁ∂≤Êâæ‰∫Ü‰∏Ä‰∫õË≥áÊñô‰æÜ‰∫ÜËß£ÂÆÉÂÄëÁöÑÂ∑ÆÂà•„ÄÇÂæå‰æÜÁôºÁèæÂÖ∂ÂØ¶ÈÄô‰∏âÂÄãÂêçË©ûÊúâÂæàÂ§ßÁöÑ‰∏çÂêåÔºåËÄåÂÖ∂Ê¶ÇÂøµ‰πüÂΩ±Èüø‰∫ÜÂì≤Â≠∏ÂÆ∂ÁöÑÊÄùÊÉ≥„ÄÇ\n",
      "‰ª•‰∏ãÂ∞±‰æÜË™™Ë™™‰∏âËÄÖ‰πãÈñìÁöÑÈóú‰øÇÔºö\n",
      "Êú¨Êàë(Id)ÔºöÊòØ‰∫∫Âá∫ÁîüÊôÇÊúÄÂéüÂßãÁöÑÈÉ®ÂàÜÔºå‰πüÊòØ‰∫∫ÁöÑÊÖæÊúõ„ÄÅË°ùÂãïÂíåÊú¨ËÉΩ„ÄÇÂÆÉÂè™Â∞çÁ´ãÂç≥ÊªøË∂≥ÁöÑÈúÄÊ±ÇÊúâÊâÄÂèçÊáâÔºå‰∏î‰∏çÂèóÊéßÂà∂ÔºåÊâÄ‰ª•Â∏∏ÊúÉÈÅïËÉåÈÅìÂæ∑ÊàñÁ§æÊúÉË¶èÁØÑËÄåÂÅöÂá∫ÂÇ∑ÂÆ≥ÁöÑË°åÁÇ∫„ÄÇ\n",
      "Ëá™Êàë(Ego):„ÄåËá™Êàë„ÄçÊòØ‰∏ÄÂÄãÊúâÊÑüÊÉÖÁöÑ‰∫∫Ê†ºÁµêÊßã„ÄÇÂÆÉÊòØÈÄèÈÅéÂ≠∏ÁøíÂíåËßÄÂØüËÄåÂΩ¢ÊàêÁöÑ„ÄÇÂú®ÊàêÈï∑ÁöÑÈÅéÁ®ã‰∏≠ÔºåÊàëÂÄëÈÄèÈÅéËàáÁà∂ÊØçÂíåÂÖ∂‰ªñÊàê‰∫∫Êé•Ëß∏‰æÜÁç≤ÂæóÂ∞ç‰ªñ‰∫∫ÁöÑ"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   29002.93 ms\n",
      "llama_print_timings:      sample time =      89.03 ms /   256 runs   (    0.35 ms per token,  2875.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   28180.56 ms /    32 tokens (  880.64 ms per token,     1.14 tokens per second)\n",
      "llama_print_timings:        eval time =   39915.25 ms /   255 runs   (  156.53 ms per token,     6.39 tokens per second)\n",
      "llama_print_timings:       total time =   69034.77 ms /   287 tokens\n"
     ]
    }
   ],
   "source": [
    "llm.chat_prompt_qa('„ÄåÊú¨Êàë(Id)„ÄÅËá™Êàë(Ego)„ÄÅË∂ÖÊàë(Superego)„ÄçÊòØË™∞ÊèêÂá∫ÁöÑ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new RetrievalQA chain...\u001b[0m\n",
      " ‰∫ûÈáåÊñØÂ§öÂæ∑„ÄÇ\n",
      "\n",
      "Yi‚Äôs, i = 1, ..., m. This model can also be used to develop Bayes predictors of Œ∏i‚Äôs. For small\n",
      "\n",
      "ÂÄãÂèØÂÆâÁ©©ÁöÑÂà∫ÊøÄËá¥‰∏≠Ôºå‰ΩøÊàëÂÄëËÉΩÈõÜ‰∏≠Ê≥®ÊÑèÂäõÂú®ÊúâÈôêÁöÑÁØÑÂúçÁí∞Â¢É‰∏≠„ÄÇÈÄô‰πüËÉΩÂæûÂÅ¥Èù¢‰∏ä\n",
      "È©óË≠âÁÇ∫‰ΩïÈùíÂ∞ëÂπ¥ÂÇæÂêëÊñºÊªëÊâãÊ©üÂà∞Áù°ËëóÂâçÁöÑÊúÄÂæå‰∏ÄÂàªÔºåÂõ†ÁÇ∫ÈÄôÊ®£ËÉΩÁç≤ÁöÑÂÆâÂÖ®ÊÑüÂíåÂæû\n",
      "‰∏≠Áç≤ÂæóÂ∞ëË®±ÂæóÂø´Ê®Ç„ÄÇ\n",
      "ÂíåÂãï‰ΩúÔºå‰Ωø„ÄåËá™Êàë„ÄçË∂ÖË∂ä‰∫Ü„ÄåÊú¨Êàë„ÄçÂíå„ÄåË∂ÖÊàë„ÄçÔºåËÄåÊ∂àÂº≠‰∫ÜÁñºÁóõ‰ª•ÂèäÊ∏õÂ∞ëÁîüÁêÜ‰∏äÁöÑÊÖæÊúõÔºå\n",
      "Âõ†Ê≠§Âú®Á∂≤Ë∑Ø‰∏äÊôÇÂ∏∏ÂèØ‰ª•ÁúãÂà∞Á∑¥ÁëúÁèàÁöÑ‰∫∫ËÉΩÂ∞áË∫´È´îË∂ÖÂá∫Â∏∏ÁêÜÁöÑÁøªÊë∫ÊàñÊòØÁ¶ÅÈ£üÂæàÈï∑‰∏Ä\n",
      "ÊÆµÊôÇÈñì„ÄÇ\n",
      "Êú¨Êõ∏Â∏åÊúõ„ÄåËá™Êàë„Äç‰ΩîÈ†òË∫´È´îÁöÑ‰∏ªÂ∞éÊ¨äÔºå‰∏ªË¶ÅÁî±ÊñºÈÄ≤ÂÖ•Ê≠§ÁãÄÊÖãÈÄöÂ∏∏ÊòØÈúÄË¶ÅÊúâÊÑèË≠òÂú∞Â∞á\n",
      "‰∫∫È°ûÁöÑËÇâÈ´îÂíåÂøÉÊô∫ÁôºÊèÆÂà∞Ê•µÈôêÔºåÂéªÂÆåÊàêÊüê‰ª∂‰πüÈõ£Â∫¶ÊàñÊúâÂÉπÂÄºÁöÑ‰∫ãÊÉÖÔºåÈÄ≤ËÄåÁ∑†ÈÄ†Âá∫\n",
      "„ÄåÊúÄÂÑ™È´îÈ©ó„Äç‰∏¶Âæû‰∏≠Áç≤ÂæóÁ¥îÁ≤πÁöÑÂø´Ê®ÇÔºåËÄåÂÖ∂‰∏≠ÊúâÊÑèË≠òÂú∞Â∞áËÇâÈ´îÂíåÂøÉÊô∫ÁôºÊèÆÂà∞Ê•µËá¥ÁöÑ\n",
      "ÁãÄÊÖãÂ∞±ÊòØÂøÉÊµÅÁãÄÊÖã„ÄÇÂõ†Ê≠§‰ª•ÂøÉÊµÅÁãÄÊÖãÂâµÈÄ†ÊúÄÂÑ™È´îÈ©óÊàñË®±"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =   31460.71 ms\n",
      "llama_print_timings:      sample time =      88.03 ms /   256 runs   (    0.34 ms per token,  2908.10 tokens per second)\n",
      "llama_print_timings: prompt eval time =   61288.86 ms /   839 tokens (   73.05 ms per token,    13.69 tokens per second)\n",
      "llama_print_timings:        eval time =   34103.40 ms /   255 runs   (  133.74 ms per token,     7.48 tokens per second)\n",
      "llama_print_timings:       total time =   96354.32 ms /  1094 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "llm.chat_rag('„ÄåÊú¨Êàë(Id)„ÄÅËá™Êàë(Ego)„ÄÅË∂ÖÊàë(Superego)„ÄçÊòØË™∞ÊèêÂá∫ÁöÑ?',dataloader.get_chromadb())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for StuffDocumentsChain\n__root__\n  document_variable_name context was not found in llm_chain input_variables: ['question'] (type=value_error)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValidationError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat_promptrag\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m„ÄåÊú¨Êàë(Id)„ÄÅËá™Êàë(Ego)„ÄÅË∂ÖÊàë(Superego)„ÄçÊòØË™∞ÊèêÂá∫ÁöÑ?\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_chromadb\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/rag_llama2-7b-chat/LLMLoader.py:101\u001b[0m, in \u001b[0;36mTestLLM.chat_promptrag\u001b[0;34m(self, query, vectordb)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchat_promptrag\u001b[39m(\u001b[38;5;28mself\u001b[39m,query,vectordb):\n\u001b[1;32m    100\u001b[0m     retriever \u001b[38;5;241m=\u001b[39m vectordb\u001b[38;5;241m.\u001b[39mas_retriever()\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_retriever \u001b[38;5;241m=\u001b[39m \u001b[43mRetrievalQA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_chain_type\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstuff\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m        \u001b[49m\u001b[43mretriever\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretriever\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchain_type_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprompt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mPromptSelector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdefualt_rag_template\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalue\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_retriever\u001b[38;5;241m.\u001b[39minvoke(query)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/langchain/chains/retrieval_qa/base.py:106\u001b[0m, in \u001b[0;36mBaseRetrievalQA.from_chain_type\u001b[0;34m(cls, llm, chain_type, chain_type_kwargs, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Load chain from chain type.\"\"\"\u001b[39;00m\n\u001b[1;32m    105\u001b[0m _chain_type_kwargs \u001b[38;5;241m=\u001b[39m chain_type_kwargs \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[0;32m--> 106\u001b[0m combine_documents_chain \u001b[38;5;241m=\u001b[39m \u001b[43mload_qa_chain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchain_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchain_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_chain_type_kwargs\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m(combine_documents_chain\u001b[38;5;241m=\u001b[39mcombine_documents_chain, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/langchain/chains/question_answering/chain.py:249\u001b[0m, in \u001b[0;36mload_qa_chain\u001b[0;34m(llm, chain_type, verbose, callback_manager, **kwargs)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chain_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m loader_mapping:\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    246\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGot unsupported chain type: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchain_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    247\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloader_mapping\u001b[38;5;241m.\u001b[39mkeys()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    248\u001b[0m     )\n\u001b[0;32m--> 249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloader_mapping\u001b[49m\u001b[43m[\u001b[49m\u001b[43mchain_type\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/langchain/chains/question_answering/chain.py:81\u001b[0m, in \u001b[0;36m_load_stuff_chain\u001b[0;34m(llm, prompt, document_variable_name, verbose, callback_manager, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m     73\u001b[0m llm_chain \u001b[38;5;241m=\u001b[39m LLMChain(\n\u001b[1;32m     74\u001b[0m     llm\u001b[38;5;241m=\u001b[39mllm,\n\u001b[1;32m     75\u001b[0m     prompt\u001b[38;5;241m=\u001b[39m_prompt,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     78\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39mcallbacks,\n\u001b[1;32m     79\u001b[0m )\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# TODO: document prompt\u001b[39;00m\n\u001b[0;32m---> 81\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mStuffDocumentsChain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     82\u001b[0m \u001b[43m    \u001b[49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     83\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocument_variable_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     84\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m     85\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallback_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.9/site-packages/pydantic/v1/main.py:341\u001b[0m, in \u001b[0;36mBaseModel.__init__\u001b[0;34m(__pydantic_self__, **data)\u001b[0m\n\u001b[1;32m    339\u001b[0m values, fields_set, validation_error \u001b[38;5;241m=\u001b[39m validate_model(__pydantic_self__\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m, data)\n\u001b[1;32m    340\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m validation_error:\n\u001b[0;32m--> 341\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m validation_error\n\u001b[1;32m    342\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    343\u001b[0m     object_setattr(__pydantic_self__, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m__dict__\u001b[39m\u001b[38;5;124m'\u001b[39m, values)\n",
      "\u001b[0;31mValidationError\u001b[0m: 1 validation error for StuffDocumentsChain\n__root__\n  document_variable_name context was not found in llm_chain input_variables: ['question'] (type=value_error)"
     ]
    }
   ],
   "source": [
    "llm.chat_promptrag('„ÄåÊú¨Êàë(Id)„ÄÅËá™Êàë(Ego)„ÄÅË∂ÖÊàë(Superego)„ÄçÊòØË™∞ÊèêÂá∫ÁöÑ?',dataloader.get_chromadb())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='‚ñ°\\n29', metadata={'author': '', 'creationDate': \"D:20220304071224-05'00'\", 'creator': 'LaTeX with hyperref package', 'file_path': 'data/arxiv_finance/1610.08909.pdf', 'format': 'PDF 1.4', 'keywords': '', 'modDate': \"D:20220304071224-05'00'\", 'page': 28, 'producer': 'dvips + GPL Ghostscript GIT PRERELEASE 9.22', 'source': 'data/arxiv_finance/1610.08909.pdf', 'subject': '', 'title': '', 'total_pages': 50, 'trapped': ''}),\n",
       " Document(page_content='(2,ùëõ) + 1| ‚àí|ùúÅùëò\\n(2,ùëõ)|)} ‚àö‚àÜùë°] ,', metadata={'author': 'svetlozar rachev', 'creationDate': \"D:20230329152933-05'00'\", 'creator': 'Microsoft¬Æ Word 2019', 'file_path': 'data/arxiv_finance/2303.17014.pdf', 'format': 'PDF 1.7', 'keywords': '', 'modDate': \"D:20230329152933-05'00'\", 'page': 20, 'producer': 'Microsoft¬Æ Word 2019', 'source': 'data/arxiv_finance/2303.17014.pdf', 'subject': '', 'title': '', 'total_pages': 49, 'trapped': ''}),\n",
       " Document(page_content='(15)\\nzr\\nx(a) := (1‚àír)ux(a)\\nux\\n+rmx(a) ‚àè\\nad‚ààa\\nux(ad)\\nux\\n.\\n19', metadata={'author': 'Srinivas Arigapudi ; Omer Edhan ; Yuval Heller ; Ziv Hellman', 'creationDate': 'D:20231114033845Z', 'creator': 'LaTeX with hyperref', 'file_path': 'data/arxiv_finance/2205.00278.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20231114033845Z', 'page': 18, 'producer': 'pdfTeX-1.40.25', 'source': 'data/arxiv_finance/2205.00278.pdf', 'subject': '', 'title': 'Mentors and Recombinators: Multi-Dimensional Social Learning', 'total_pages': 58, 'trapped': ''}),\n",
       " Document(page_content='œµ0, Àú\\nœµ‚àí1, . . . do we prefer to use a different letter for them. Note\\n15', metadata={'author': 'Beutner Heinemann Smeekes', 'creationDate': 'D:20230816005521Z', 'creator': 'LaTeX with hyperref', 'file_path': 'data/arxiv_finance/1808.09125.pdf', 'format': 'PDF 1.5', 'keywords': '', 'modDate': 'D:20230816005521Z', 'page': 14, 'producer': 'pdfTeX-1.40.25', 'source': 'data/arxiv_finance/1808.09125.pdf', 'subject': '', 'title': 'Bootstrapping Risk Measures', 'total_pages': 117, 'trapped': ''})]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataloader.chroma_db.similarity_search('Ë∑üÊàëÊé®Ëñ¶Ë≤°Á∂ìÁõ∏ÈóúË´ñÊñá')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TokenizerSelector Comparison\n",
    "Language Comparison\n",
    "Model Comparison\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
